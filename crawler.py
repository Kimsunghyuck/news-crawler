"""
ë©€í‹° ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ëª¨ë“ˆ
ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ í¬ë¡¤ë§í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
import os
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional
import logging
from bs4 import BeautifulSoup
import re

# í•œêµ­ ì‹œê°„ëŒ€ (KST = UTC+9)
KST = timezone(timedelta(hours=9))

def get_kst_now():
    """í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

from config import (
    NEWS_SOURCES, HEADERS, REQUEST_DELAY, REQUEST_TIMEOUT,
    DATA_DIR, NEWS_JSON_TEMPLATE, LOGS_DIR, LOG_FILE,
    MAX_RETRIES, RETRY_DELAY, AUTO_GENERATE_REPORT,
    CATEGORY_EN_MAP, SOURCE_EN_MAP, COMBINED_REPORT_TEMPLATE
)
import parser


# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    os.makedirs(LOGS_DIR, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


logger = setup_logging()


def extract_article_image(article_url: str, source: str) -> str:
    """
    ê¸°ì‚¬ URLì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        article_url: ê¸°ì‚¬ URL
        source: ë‰´ìŠ¤ ì†ŒìŠ¤ ì´ë¦„
        
    Returns:
        ì´ë¯¸ì§€ URL ë¬¸ìì—´ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    """
    try:
        response = requests.get(article_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        
        image_url = ""
        
        # Open Graph ì´ë¯¸ì§€ ë©”íƒ€ íƒœê·¸ (ê°€ì¥ ì¼ë°˜ì )
        og_image = soup.find('meta', property='og:image')
        if og_image:
            image_url = og_image.get('content', '')
        
        # Twitter ì¹´ë“œ ì´ë¯¸ì§€
        if not image_url:
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image:
                image_url = twitter_image.get('content', '')
        
        # ê¸°ì‚¬ ë³¸ë¬¸ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
        if not image_url:
            article_body = soup.find(['article', 'div'], class_=re.compile(r'article|content|body'))
            if article_body:
                img_tag = article_body.find('img')
                if img_tag:
                    image_url = img_tag.get('src', '') or img_tag.get('data-src', '')
        
        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
        if image_url and image_url.startswith('/'):
            from urllib.parse import urlparse
            parsed = urlparse(article_url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
            image_url = f"{base_url}{image_url}"
        
        return image_url
        
    except Exception as e:
        logger.debug(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {e}")
        return ""


def fetch_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:
    """
    URLì—ì„œ HTML í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        url: í¬ë¡¤ë§í•  URL
        retries: ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        HTML ë¬¸ìì—´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    for attempt in range(retries):
        try:
            logger.info(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹œë„ ({attempt + 1}/{retries}): {url}")
            response = requests.get(
                url,
                headers=HEADERS,
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            logger.info(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {e}")
            
            if attempt < retries - 1:
                logger.info(f"{RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
                return None
    
    return None


def get_category_source_path(category: str, source: str, date: str, is_report: bool = False) -> str:
    """
    ì¹´í…Œê³ ë¦¬ì™€ ì†ŒìŠ¤ì— ë”°ë¥¸ íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        category: ì¹´í…Œê³ ë¦¬ (í•œê¸€)
        source: ì†ŒìŠ¤ ì´ë¦„ (í•œê¸€)
        date: ë‚ ì§œ (YYYY-MM-DD)
        is_report: ë³´ê³ ì„œ ê²½ë¡œ ì—¬ë¶€
        
    Returns:
        íŒŒì¼ ê²½ë¡œ
    """
    category_en = CATEGORY_EN_MAP.get(category, category.lower())
    source_en = SOURCE_EN_MAP.get(source, source.lower().replace(' ', '_'))
    
    if is_report:
        from config import REPORT_TEMPLATE
        return REPORT_TEMPLATE.format(category=category_en, source=source_en, date=date)
    else:
        return NEWS_JSON_TEMPLATE.format(category=category_en, source=source_en, date=date)


def get_today_json_file():
    """ì˜¤ëŠ˜ ë‚ ì§œì˜ í†µí•© JSON íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (í•˜ìœ„ í˜¸í™˜ì„±)."""
    today = datetime.now().strftime('%Y-%m-%d')
    # í†µí•© JSONì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    return f"{DATA_DIR}/news_{today}.json"


def load_existing_news_by_source(category: str, source: str) -> List[Dict[str, str]]:
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬/ì†ŒìŠ¤ì˜ ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        category: ì¹´í…Œê³ ë¦¬
        source: ì†ŒìŠ¤ ì´ë¦„
        
    Returns:
        ê¸°ì¡´ ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    today = datetime.now().strftime('%Y-%m-%d')
    json_file = get_category_source_path(category, source, today)
    
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"[{source}] ê¸°ì¡´ ë‰´ìŠ¤ {len(data)}ê°œ ë¡œë“œë¨")
                return data
        except Exception as e:
            logger.error(f"[{source}] ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    else:
        logger.info(f"[{source}] ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ")
        return []


def save_news_by_source(category: str, source: str, news_items: List[Dict[str, str]]):
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬/ì†ŒìŠ¤ì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        category: ì¹´í…Œê³ ë¦¬
        source: ì†ŒìŠ¤ ì´ë¦„
        news_items: ì €ì¥í•  ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    today = datetime.now().strftime('%Y-%m-%d')
    json_file = get_category_source_path(category, source, today)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(json_file), exist_ok=True)
    
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(news_items, f, ensure_ascii=False, indent=2)
        
        logger.info(f"[{source}] ë‰´ìŠ¤ {len(news_items)}ê°œë¥¼ {json_file}ì— ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"[{source}] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")


def load_all_news() -> List[Dict[str, str]]:
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬/ì†ŒìŠ¤ì˜ ë‰´ìŠ¤ë¥¼ ë¡œë“œí•˜ì—¬ í†µí•©í•©ë‹ˆë‹¤.
    
    Returns:
        í†µí•©ëœ ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    all_news = []
    today = datetime.now().strftime('%Y-%m-%d')
    
    for category, sources in NEWS_SOURCES.items():
        for source_config in sources:
            source_name = source_config['name']
            json_file = get_category_source_path(category, source_name, today)
            
            if os.path.exists(json_file):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        all_news.extend(data)
                except Exception as e:
                    logger.error(f"[{source_name}] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return all_news


def merge_news(existing_news: List[Dict[str, str]], new_news: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ê¸°ì¡´ ë‰´ìŠ¤ì™€ ìƒˆ ë‰´ìŠ¤ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤ (ì¤‘ë³µ ì œê±°).
    
    Args:
        existing_news: ê¸°ì¡´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        new_news: ìƒˆë¡œ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³‘í•©ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # URLì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    news_dict = {item['url']: item for item in existing_news}
    
    new_count = 0
    for item in new_news:
        if item['url'] not in news_dict:
            news_dict[item['url']] = item
            new_count += 1
        else:
            # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ (ìŠ¤í¬ë© ì‹œê°„ ë“±)
            news_dict[item['url']].update(item)
    
    logger.info(f"ìƒˆë¡œìš´ ë‰´ìŠ¤ {new_count}ê°œ ë°œê²¬")
    
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬
    merged_news = list(news_dict.values())
    merged_news.sort(key=lambda x: x.get('date', ''), reverse=True)
    
    return merged_news


def crawl_news() -> bool:
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info("=" * 60)
    logger.info("ë©€í‹° ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    kst_now = get_kst_now()
    logger.info(f"í˜„ì¬ ì‹œê°„ (KST): {kst_now.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ì‹¤í–‰ ì‹œê°„ëŒ€: {kst_now.strftime('%p %Iì‹œ')} KST".replace('AM', 'ì˜¤ì „').replace('PM', 'ì˜¤í›„'))
    logger.info("=" * 60)
    
    try:
        all_news_count = 0
        category_stats = {}
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ í¬ë¡¤ë§
        for category, sources in NEWS_SOURCES.items():
            if not sources:  # ì†ŒìŠ¤ê°€ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°
                logger.info(f"'{category}' ì¹´í…Œê³ ë¦¬: ì„¤ì •ëœ ë‰´ìŠ¤ ì†ŒìŠ¤ ì—†ìŒ")
                continue
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {category}")
            logger.info(f"{'='*60}")
            
            category_stats[category] = 0
            
            for source_config in sources:
                source_name = source_config['name']
                url = source_config['url']
                parser_name = source_config['parser']
                max_articles = source_config.get('max_articles', 20)
                
                logger.info(f"\ní¬ë¡¤ë§ ì†ŒìŠ¤: {source_name}")
                logger.info(f"URL: {url}")
                
                # 1. í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                html_content = fetch_page(url)
                if not html_content:
                    logger.error(f"{source_name} í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                # 2. ì ì ˆí•œ íŒŒì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                try:
                    parser_func = getattr(parser, f'parse_{parser_name}')
                except AttributeError:
                    logger.error(f"íŒŒì„œ í•¨ìˆ˜ 'parse_{parser_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                # 3. HTML íŒŒì‹±
                logger.info(f"HTML íŒŒì‹± ì¤‘... (íŒŒì„œ: {parser_name})")
                
                # max_articles ì¸ìë¥¼ ë°›ëŠ” íŒŒì„œë“¤
                if parser_name in ['donga_politics', 'chosun_politics', 'joongang_politics',
                                    'donga_sports', 'chosun_sports', 'joongang_sports',
                                    'donga_economy', 'chosun_economy', 'joongang_economy']:
                    new_news = parser_func(html_content, max_articles)
                else:
                    new_news = parser_func(html_content)
                    
                logger.info(f"íŒŒì‹± ì™„ë£Œ: {len(new_news)}ê°œ ë‰´ìŠ¤ í•­ëª© ë°œê²¬")
                
                # ê° ë‰´ìŠ¤ í•­ëª©ì˜ ê¸°ì‚¬ URLì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                for item in new_news:
                    if not item.get('image_url'):  # ì´ë¯¸ì§€ URLì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ
                        image_url = extract_article_image(item['url'], source_name)
                        item['image_url'] = image_url
                        time.sleep(0.5)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
                
                # ì¹´í…Œê³ ë¦¬ì™€ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                for item in new_news:
                    item['main_category'] = category
                    if 'source' not in item:
                        item['source'] = source_name
                
                # 4. ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ (ì†ŒìŠ¤ë³„)
                existing_news = load_existing_news_by_source(category, source_name)
                
                # 5. ë³‘í•©
                merged_news = merge_news(existing_news, new_news)
                
                # 6. ì†ŒìŠ¤ë³„ë¡œ ì €ì¥
                save_news_by_source(category, source_name, merged_news)
                
                all_news_count += len(merged_news)
                category_stats[category] += len(merged_news)
                
                # Rate limiting
                time.sleep(REQUEST_DELAY)
        
        if all_news_count == 0:
            logger.warning("íŒŒì‹±ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        logger.info("\n" + "=" * 60)
        logger.info("í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ì´ ë‰´ìŠ¤ ê°œìˆ˜: {all_news_count}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        logger.info("\nì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ê°œìˆ˜:")
        for cat, count in sorted(category_stats.items()):
            if count > 0:
                logger.info(f"  - {cat}: {count}ê°œ")
        
        logger.info("=" * 60)
        
        # ìë™ ë³´ê³ ì„œ ìƒì„±
        if AUTO_GENERATE_REPORT:
            try:
                logger.info("=" * 60)
                logger.info("ğŸ“ í†µí•© ë³´ê³ ì„œ ìë™ ìƒì„± ì‹œì‘")
                logger.info("=" * 60)
                
                from report_generator import generate_combined_report
                today = datetime.now().strftime('%Y-%m-%d')
                report_file = generate_combined_report(today)
                
                logger.info(f"âœ… í†µí•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
                logger.info("=" * 60)
                
            except Exception as e:
                logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        
        # íŠ¸ë Œë“œ ë°ì´í„° ìë™ ìƒì„±
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„° ìƒì„± ì‹œì‘")
            logger.info("=" * 60)
            
            from analyzer import save_trend_data
            today = get_kst_now().strftime('%Y-%m-%d')
            trend_file = save_trend_data(today)
            
            if trend_file:
                logger.info(f"âœ… íŠ¸ë Œë“œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {trend_file}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"íŠ¸ë Œë“œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        
        return True
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë‹¨ì¼ ì‹¤í–‰ìš©"""
    success = crawl_news()
    
    if success:
        print(f"\nâœ“ í¬ë¡¤ë§ ì„±ê³µ! ë°ì´í„°ëŠ” ì¹´í…Œê³ ë¦¬/ì†ŒìŠ¤ë³„ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"  - ë°ì´í„°: {DATA_DIR}/{{category}}/{{source}}/")
        print(f"  - ë³´ê³ ì„œ: {COMBINED_REPORT_TEMPLATE.split('/')[0]}/combined/")
    else:
        print("\nâœ— í¬ë¡¤ë§ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
