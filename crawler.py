"""
Anthropic ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ëª¨ë“ˆ
ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Optional
import logging

from config import (
    NEWS_URL, HEADERS, REQUEST_DELAY, REQUEST_TIMEOUT,
    DATA_DIR, NEWS_JSON_TEMPLATE, LOGS_DIR, LOG_FILE,
    MAX_RETRIES, RETRY_DELAY, AUTO_GENERATE_REPORT
)
from parser import parse_news_page


# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    os.makedirs(LOGS_DIR, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


logger = setup_logging()


def fetch_page(url: str, retries: int = MAX_RETRIES) -> Optional[str]:
    """
    URLì—ì„œ HTML í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        url: í¬ë¡¤ë§í•  URL
        retries: ì¬ì‹œë„ íšŸìˆ˜
        
    Returns:
        HTML ë¬¸ìì—´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    for attempt in range(retries):
        try:
            logger.info(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹œë„ ({attempt + 1}/{retries}): {url}")
            response = requests.get(
                url,
                headers=HEADERS,
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            logger.info(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {url}")
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {e}")
            
            if attempt < retries - 1:
                logger.info(f"{RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
                return None
    
    return None


def get_today_json_file():
    """ì˜¤ëŠ˜ ë‚ ì§œì˜ JSON íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    today = datetime.now().strftime('%Y-%m-%d')
    return NEWS_JSON_TEMPLATE.format(date=today)


def load_existing_news() -> List[Dict[str, str]]:
    """
    ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        ê¸°ì¡´ ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    json_file = get_today_json_file()
    
    if os.path.exists(json_file):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"ê¸°ì¡´ ë‰´ìŠ¤ {len(data)} ê°œ ë¡œë“œë¨")
                return data
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    else:
        logger.info("ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ")
        return []


def save_news(news_items: List[Dict[str, str]]):
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        news_items: ì €ì¥í•  ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸
    """
    os.makedirs(DATA_DIR, exist_ok=True)
    
    today = datetime.now().strftime('%Y-%m-%d')
    json_file = NEWS_JSON_TEMPLATE.format(date=today)
    
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(news_items, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë‰´ìŠ¤ {len(news_items)}ê°œë¥¼ {json_file}ì— ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")


def merge_news(existing_news: List[Dict[str, str]], new_news: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ê¸°ì¡´ ë‰´ìŠ¤ì™€ ìƒˆ ë‰´ìŠ¤ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤ (ì¤‘ë³µ ì œê±°).
    
    Args:
        existing_news: ê¸°ì¡´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        new_news: ìƒˆë¡œ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³‘í•©ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    # URLì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    news_dict = {item['url']: item for item in existing_news}
    
    new_count = 0
    for item in new_news:
        if item['url'] not in news_dict:
            news_dict[item['url']] = item
            new_count += 1
        else:
            # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ (ìŠ¤í¬ë© ì‹œê°„ ë“±)
            news_dict[item['url']].update(item)
    
    logger.info(f"ìƒˆë¡œìš´ ë‰´ìŠ¤ {new_count}ê°œ ë°œê²¬")
    
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬
    merged_news = list(news_dict.values())
    merged_news.sort(key=lambda x: x.get('date', ''), reverse=True)
    
    return merged_news


def crawl_news() -> bool:
    """
    Anthropic ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    logger.info("=" * 60)
    logger.info("ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)
    
    try:
        # 1. ë‰´ìŠ¤ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        html_content = fetch_page(NEWS_URL)
        if not html_content:
            logger.error("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # 2. HTML íŒŒì‹±
        logger.info("HTML íŒŒì‹± ì¤‘...")
        new_news = parse_news_page(html_content)
        logger.info(f"íŒŒì‹± ì™„ë£Œ: {len(new_news)}ê°œ ë‰´ìŠ¤ í•­ëª© ë°œê²¬")
        
        if not new_news:
            logger.warning("íŒŒì‹±ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # 3. ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ
        existing_news = load_existing_news()
        
        # 4. ë³‘í•©
        merged_news = merge_news(existing_news, new_news)
        
        # 5. ì €ì¥
        save_news(merged_news)
        
        logger.info("=" * 60)
        logger.info("í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(merged_news)}")
        logger.info("=" * 60)
        
        # Rate limiting
        time.sleep(REQUEST_DELAY)
        
        # ìë™ ë³´ê³ ì„œ ìƒì„±
        if AUTO_GENERATE_REPORT:
            try:
                logger.info("=" * 60)
                logger.info("ğŸ“ ë³´ê³ ì„œ ìë™ ìƒì„± ì‹œì‘")
                logger.info("=" * 60)
                
                from report_generator import generate_report_from_json
                json_file = get_today_json_file()
                report_file, _ = generate_report_from_json(json_file)
                
                logger.info(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
                logger.info("=" * 60)
                
            except Exception as e:
                logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        
        return True
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë‹¨ì¼ ì‹¤í–‰ìš©"""
    success = crawl_news()
    
    json_file = get_today_json_file()
    
    if success:
        print(f"\nâœ“ í¬ë¡¤ë§ ì„±ê³µ! ë°ì´í„°ëŠ” {json_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâœ— í¬ë¡¤ë§ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
