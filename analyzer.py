"""
ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ëª¨ë“ˆ
ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import json
import os
import re
from datetime import datetime, timezone, timedelta
from collections import Counter
from typing import List, Dict
from config import DATA_DIR

# í•œêµ­ ì‹œê°„ëŒ€ (KST = UTC+9)
KST = timezone(timedelta(hours=9))

def get_kst_now():
    """í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(KST)

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ë¶„ì„ì—ì„œ ì œì™¸í•  ë‹¨ì–´ë“¤)
STOPWORDS = {
    'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ',
    'ë˜í•œ', 'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì´ëŠ”', 'ê²ƒì€', 'ê²ƒì´', 'ê²ƒì„',
    'ìš°ë¦¬', 'ì €í¬', 'ì´ë²ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì˜¬í•´', 'ì‘ë…„', 'ë‚´ë…„',
    'í†µí•´', 'ìœ„í•´', 'ëŒ€í•´', 'ê´€ë ¨', 'ë”°ë¥´ë©´', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ë§í–ˆë‹¤',
    'ì´ë¼ê³ ', 'ë¼ê³ ', 'í•œë‹¤', 'í•œë‹¤ê³ ', 'í–ˆë‹¤', 'í–ˆë‹¤ê³ ', 'ë ', 'ë ê¹Œ', 'ê¸°ì',
    'ë‰´ìŠ¤', 'ì†ë³´', 'ë‹¨ë…', 'íŠ¹ì¢…', 'ì·¨ì¬', 'ë³´ë„', 'ë°œí‘œ', 'ê³µê°œ', 'í™•ì¸'
}

def extract_korean_nouns(text: str, min_length: int = 2, max_length: int = 10) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜).
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        min_length: ìµœì†Œ ë‹¨ì–´ ê¸¸ì´
        max_length: ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´
    
    Returns:
        ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
    pattern = f'[ê°€-í£]{{{min_length},{max_length}}}'
    words = re.findall(pattern, text)
    
    # ë¶ˆìš©ì–´ ì œê±°
    filtered_words = [w for w in words if w not in STOPWORDS]
    
    return filtered_words


def analyze_daily_keywords(date: str = None, top_n: int = 20) -> List[Dict[str, any]]:
    """
    íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        date: ë¶„ì„í•  ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        top_n: ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë°˜í™˜
    
    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: [{"word": "í‚¤ì›Œë“œ", "count": íšŸìˆ˜}, ...]
    """
    if date is None:
        date = get_kst_now().strftime('%Y-%m-%d')
    
    all_keywords = []
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬/ì†ŒìŠ¤ì˜ ë‰´ìŠ¤ íŒŒì¼ íƒìƒ‰
    for category_dir in os.listdir(DATA_DIR):
        category_path = os.path.join(DATA_DIR, category_dir)
        
        if not os.path.isdir(category_path):
            continue
        
        for source_dir in os.listdir(category_path):
            source_path = os.path.join(category_path, source_dir)
            
            if not os.path.isdir(source_path):
                continue
            
            # í•´ë‹¹ ë‚ ì§œì˜ ë‰´ìŠ¤ íŒŒì¼ ì°¾ê¸°
            news_file = os.path.join(source_path, f'news_{date}.json')
            
            if os.path.exists(news_file):
                try:
                    with open(news_file, 'r', encoding='utf-8') as f:
                        news_items = json.load(f)
                    
                    # ê° ë‰´ìŠ¤ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    for item in news_items:
                        title = item.get('title', '')
                        keywords = extract_korean_nouns(title)
                        all_keywords.extend(keywords)
                        
                except Exception as e:
                    print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({news_file}): {e}")
                    continue
    
    # ë¹ˆë„ ë¶„ì„
    keyword_counts = Counter(all_keywords)
    
    # ìƒìœ„ Nê°œ ì¶”ì¶œ
    top_keywords = [
        {"word": word, "count": count}
        for word, count in keyword_counts.most_common(top_n)
    ]
    
    return top_keywords


def analyze_category_keywords(category: str, date: str = None, top_n: int = 10) -> List[Dict[str, any]]:
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        category: ì¹´í…Œê³ ë¦¬ (politics, sports, economy ë“±)
        date: ë¶„ì„í•  ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜
        top_n: ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë°˜í™˜
    
    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if date is None:
        date = get_kst_now().strftime('%Y-%m-%d')
    
    all_keywords = []
    category_path = os.path.join(DATA_DIR, category)
    
    if not os.path.isdir(category_path):
        return []
    
    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ì†ŒìŠ¤ íƒìƒ‰
    for source_dir in os.listdir(category_path):
        source_path = os.path.join(category_path, source_dir)
        
        if not os.path.isdir(source_path):
            continue
        
        news_file = os.path.join(source_path, f'news_{date}.json')
        
        if os.path.exists(news_file):
            try:
                with open(news_file, 'r', encoding='utf-8') as f:
                    news_items = json.load(f)
                
                for item in news_items:
                    title = item.get('title', '')
                    keywords = extract_korean_nouns(title)
                    all_keywords.extend(keywords)
                    
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({news_file}): {e}")
                continue
    
    keyword_counts = Counter(all_keywords)
    top_keywords = [
        {"word": word, "count": count}
        for word, count in keyword_counts.most_common(top_n)
    ]
    
    return top_keywords


def save_trend_data(date: str = None):
    """
    íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤ (GitHub Pagesìš©).
    
    Args:
        date: ì €ì¥í•  ë‚ ì§œ, Noneì´ë©´ ì˜¤ëŠ˜
    """
    if date is None:
        date = get_kst_now().strftime('%Y-%m-%d')
    
    # ì „ì²´ í‚¤ì›Œë“œ ë¶„ì„
    daily_keywords = analyze_daily_keywords(date, top_n=20)
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¶„ì„
    categories = ['politics', 'sports', 'economy', 'society', 'international', 'culture']
    category_keywords = {}
    
    for category in categories:
        category_keywords[category] = analyze_category_keywords(category, date, top_n=5)
    
    # íŠ¸ë Œë“œ ë°ì´í„° êµ¬ì¡°
    trend_data = {
        "date": date,
        "generated_at": get_kst_now().isoformat(),
        "daily_top_keywords": daily_keywords,
        "category_keywords": category_keywords
    }
    
    # docs/data/trends/ ë””ë ‰í† ë¦¬ ìƒì„±
    trends_dir = os.path.join('docs', 'data', 'trends')
    os.makedirs(trends_dir, exist_ok=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    trend_file = os.path.join(trends_dir, f'trends_{date}.json')
    
    try:
        with open(trend_file, 'w', encoding='utf-8') as f:
            json.dump(trend_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… íŠ¸ë Œë“œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {trend_file}")
        print(f"   - ì „ì²´ í‚¤ì›Œë“œ: {len(daily_keywords)}ê°œ")
        print(f"   - ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # Top 5 í‚¤ì›Œë“œ ì¶œë ¥
        if daily_keywords:
            print(f"\nğŸ”¥ ì˜¤ëŠ˜ì˜ í•« í‚¤ì›Œë“œ:")
            for i, kw in enumerate(daily_keywords[:5], 1):
                print(f"   {i}. {kw['word']} ({kw['count']}íšŒ)")
        
        return trend_file
        
    except Exception as e:
        print(f"âŒ íŠ¸ë Œë“œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


def main():
    """íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)"""
    print("=" * 60)
    print("ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    
    today = get_kst_now().strftime('%Y-%m-%d')
    
    # íŠ¸ë Œë“œ ë°ì´í„° ìƒì„± ë° ì €ì¥
    save_trend_data(today)
    
    print("=" * 60)


if __name__ == "__main__":
    main()
